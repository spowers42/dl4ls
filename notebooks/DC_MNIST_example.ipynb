{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "import deepchem as dc\n",
    "import deepchem.models.tensorgraph.layers as layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the book uses the examples.tutorials.mnists input_data method, we will use the keras mnist data method.  First, the examples are deprecated, and might not work in the future.  Additionally this eliminates the need to manually download and store these datasets.  Also the keras version makes a nice test/train split for us.  :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download and Visualization\n",
    "The MNIST dataset is pretty clean, so there is no need for any real preprocessing like an autoencoder or the such to make the network perform well.  It might be useful for near 100% accuracy, but for this we will just use the raw input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_outputs = 10\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "one_hot = OneHotEncoder(sparse=False)\n",
    "y_train = one_hot.fit_transform(y_train.reshape(-1,1))\n",
    "y_labels = y_test\n",
    "y_test = one_hot.transform(y_test.reshape(-1,1))\n",
    "train = dc.data.NumpyDataset(X_train, y_train)\n",
    "test = dc.data.NumpyDataset(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f61e37f10f0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 0\n",
    "plt.title(label=y_labels[idx])\n",
    "plt.imshow(X_train[idx], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dc.models.TensorGraph(model_dir=\"data\")\n",
    "# Define the Input and Output layers, which are flat\n",
    "feature = layers.Feature(shape=(None, 28, 28))\n",
    "labels = layers.Label(shape=(None, expected_outputs))\n",
    "\n",
    "# General Model layers\n",
    "#make_image = layers.Reshape(shape=(None, 28, 28), in_layer=feature)\n",
    "conv2d_1 = layers.Conv2D(num_outputs=32, activation_fn=tf.nn.relu, in_layers=feature)\n",
    "conv2d_2 = layers.Conv2D(num_outputs=64, activation_fn=tf.nn.relu, in_layers=conv2d_1)\n",
    "flatten = layers.Flatten(in_layers=conv2d_2)\n",
    "dense1 =  layers.Dense(out_channels=1024, activation_fn=tf.nn.relu, in_layers=flatten)\n",
    "dense2 = layers.Dense(out_channels=expected_outputs, activation_fn=None, in_layers=dense1)\n",
    "\n",
    "# Loss and scoring\n",
    "# softmax layer takes labels and predicted values\n",
    "smce = layers.SoftMaxCrossEntropy(in_layers=[labels, dense2])\n",
    "loss = layers.ReduceMean(in_layers=smce)\n",
    "model.set_loss(loss)\n",
    "output = layers.SoftMax(in_layers=dense2)\n",
    "model.add_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.accuracy_score)\n",
    "train_scores = model.evaluate(train_dataset, [metric])\n",
    "test_scores = model.evaulate(test_dataset, [metric])\n",
    "print(train_scores, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
